---
layout: default
title: Kubeflow Part 1
permalink: /kf_01/
---

## So you want to do MLOps?
There's a lot of tools out there to do MlOps, and a lot of talks, blog posts, papers, etc telling you how to use some pretty awesome tools to do pretty awesome things. I've had to wade pretty deep into this stuff lately, and I'll say that much like having a Jira board doesn't mean you're doing Agile, installing Kubeflow (or MLFlow or Seldon or whatever) doesn't mean you're doing MLOps. MLOps is still kind of vaguely defined, but if I were to sum up my understanding right now, it would be that MLOps means having a *plan*. Seems simple enough, but I think it's easy to forget just how much a lot of us are winging it when it comes to data science. 

You sort of hold onto that basic data science framework which is some variation on Clean, Explore, Featurize, Train, Validate, Deploy. This is your first clue that there's a better way of doing things, because we all keep doing the same things over and over again. At least some of them are automatable. More importantly, they are loggable. I think sometimes we get carried away with the automation, but you can never start logging early enough. Making it easier to log what you're doing and having a method of comparing logs from one model to the next, or one project to the next, that's how you get to automation, or how you determine it's time to try something different. But everything in Machine Learning is so *statistical*, and involves so much really chunky, hard-drive-devouring optimization that just recording all the steps is intractable, which is why the whole process is distinct from DevOps. 

And the conditions for change are different, normal software you typically add features and fix bugs. Machine Learning models can run entirely as expected with zero bugs until they start being **wrong all the time**. A model being wrong isn't necessarily a bug, it was obviously right at some point, that why you deployed it. A lot of things lead to model performance degrading over time, the data changes, the labels change, the goals change. Just like we have to occasionally update our understanding of the world, so do machines. It's pretty hard for me, and most humans I've met, so have a little sympathy for the machines, they're even more rigid and literal than people.

## So get to the sick tools!!!!
I've been using Kubeflow, it's not without it's problems, the biggest one that I didn't know any Kubernetes going in. Well, that's my problem, but still, Kubernetes isn't exactly a soft learning curve. Containerization is a good way of ensuring that your code doesn't break just because it switches machines, there are a number of Kubernetes solutions that seem to fall into the category of "this will make it easier to scale up later." There's good reasons to scale things up, but there are plenty of tasks that just aren't going to really scale very quickly. I'll try to point these out as I go, because I've been lead down some very complicated solutions to simple problems and realized that the simple answer was fine. The other major problem Kubeflow has is that Google seems to hate making user friendly open-source projects, and I don't think anyone's made a Keras for Kubeflow yet.  That being said, Kubeflow itself isn't that hard to use, and it's extremely flexible, has a UI, and easily integrates with a lot of other tools, notably MLFlow.

And so for Kubeflow, you need Kubernetes, I'm going to run k3d on my mac, and maybe ina couple more posts see if I can get the windows machine we keep around to play Skyrim to come into the cluster. k3d runs k3s inside a docker container, k3s is of course a lightweight version of k8s, which in turn is a lightweight version of Kubernetes. So in short, k3d is light-light-weight Kubernetes for people who can't even be bothered to install k3s, ie: me.

We just open up a terminal and get going, installing k3d requires docker and go on your computer. I'm not going to go into installing them here.

Then we can run the k3d install script:
```shell
curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | TAG=v5.0.0 bash
...
k3d version
>> k3d version v5.0.0
>> k3s version v1.21.5-k3s1 (default)
```

kubectl: You can either curl
```shell
curl -LO "https://dl.k8s.io/release/v1.21.0/bin/darwin/arm64/kubectl"
```
or brew install:
```shell
brew install kubectl
```
Note there are separate releases for Intel or Apple chips.

kustomize: As of this writing, the way I'm installing the manifests uses kustomize, and Kubeflow says that they need 3.2.0, which is not the latest version. A binary of 3.2.0 can be found [here](https://github.com/kubernetes-sigs/kustomize/releases/download/v3.2.0/kustomize_3.2.0_darwin_amd64). Then simply move it to your usr/bin folder and make it executable

```shell
mv ~/kustomize_3.2.0_darwin_amd64 /usr/bin/kustomize
chmod +x /usr/bin/kustomize
```

Now I don't know if I need some gpu driver or not to run gpu workloads on apple M1, haven't really tried yet. So I'm going to just go ahead and spin up a cluster and fix it later. However, at this point make sure you're letting docker use at least 8GB of ram, the default is apparently 4GB, and that just wasn't enough, kubectl was throwing errors for me. There's at least some things you can edit about your cluster once you start it and other things that you have to restart the cluster to change, but as you can see, the cost of starting the cluster at this point is low:

```shell
k3d cluster create kubeflow \
--volume /Users/chrismalec/kf-mnt:/var/lib/rancher/k3s/storage \
--registry-create mycluster-registry
```

and a cluster called kubeflow is created. It currently has 1 node and any persistent volumes I create will map to some hideous file path that Rancher has set as default, but I'll fix that later.

The kubeflow manifests are available [here](https://github.com/kubeflow/manifests), just clone the repo and cd yourself inside and enter this handy line of code:

```shell
while ! kustomize build example | kubectl apply -f -; do echo "Retrying to apply resources"; sleep 10; done
```

A lot of stuff is going to scroll by the screen and some of it might appear multiple times. The README explicitly states that it's normal for some commands to have to run more than once before all the correct pods/services/etc have been spun up. The whole process typically only takes a few minutes for me, you can check that all the pods are up with 
```shell
kubectl get pod -A
```
